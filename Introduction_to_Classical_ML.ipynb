{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO8cn1yR3ChE8N0gPsPMsC6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jirayuwat12/SuperAI/blob/master/Introduction_to_Classical_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## types of machine learning \n",
        "\n",
        "1. supervised\n",
        "1. unsupervised\n",
        "1. reinforcement"
      ],
      "metadata": {
        "id": "zAasZDWtJazU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "lyfgTkcGYTfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Metrics\n",
        "- compare the output of the models\n",
        "  - errors/failures, accuracy/success\n",
        "- we want to quantify the error/accuracy of the models\n",
        "- how would you measure the error/accuracy of the following\n",
        "\n",
        "ex. AUC, Confusion, FA FR, Error, Door open correctly\n",
        "\n",
        "1. Error rate\n",
        "1. Accuracy rate\n",
        "1. Precision\n",
        "1. True positive\n",
        "1. Recall\n",
        "1. False alarm\n",
        "1. F score"
      ],
      "metadata": {
        "id": "8B4hJNExNhcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear regression"
      ],
      "metadata": {
        "id": "_7vGaKsSYP-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "eg. predicting housing price\n",
        "\n",
        "$h_θ (x_1) = θ_0 + θ_1x_{1,1}+ θ_2x_{1,2}+...  $\n",
        "\n",
        "$h_θ (x_1) = ∑ θ^Tx_i$\n",
        "\n",
        "θs are the parameter (or **weigths**)\n",
        "\n",
        "- cost/loss function\n",
        "\n",
        "  1. MSE ( mean square error ) or L2 loss\n",
        "\n",
        "  $ J(θ) = \\dfrac{1}{m} ∑ (y_i - θ^T x_i)^2 $\n",
        "\n",
        "  2. L1 loss\n",
        "\n",
        "  $ J(θ) = \\dfrac{1}{m} ∑ |y_i - θ^T x_i| $\n",
        "\n",
        "- gradient descent \n",
        "  \n",
        "  1. y = f(x)\n",
        "  1. pick start point $x_0$\n",
        "  1. move along $-\\dfrac{dy}{dx}$\n",
        "  1. $x_{n+1} = x_n - r \\times \\dfrac{dy}{dx}$\n",
        "  1. repeat till convergence\n",
        "  \n",
        "  <u>note</u> r = learning rate\n",
        "\n",
        "  - gradient of loss function\n",
        "$\n",
        "    \\dfrac{∂J}{∂θ_j} = - ∑(y_i - θ^T x_i)x_i^{(j)}\n",
        "$\n",
        "\n",
        "$\n",
        "  hence, θ_{j+1} \\impliedby θ_j + r ∑ (y_i - θ^T x_i)x_i^{(j)}\n",
        "$"
      ],
      "metadata": {
        "id": "CTcaIkHKQzaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression with non-linear features"
      ],
      "metadata": {
        "id": "W4SGnvVxZaSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "eg. predicting housing price\n",
        "\n",
        "$h_θ (x_1) = θ_0 + θ_1x_{1,1}+ θ_2x_{1,2}+... +θ_nx_{1,1}^2+θ_{n+1}x_{1,2}^2 + ... $\n",
        "\n",
        "θs are the parameter (or **weigths**)\n",
        "\n",
        "can cause **Overfitting and Underfitting**\n",
        "\n",
        "- reducing overfiting with regularization\n",
        "\n",
        "  1. L1 regularization\n",
        "  $$\n",
        "  \\begin{split}\n",
        "    Regularized Loss &= Original loss + regularization\\\\\n",
        "    &=\\dfrac{1}{m} ∑(y_i - θ^T x_i)^2 + γ ∑ |θ_j|\n",
        "  \\end{split}\n",
        "  $$\n",
        "  L1 does feature selection ( most number be 0 )\n",
        "  2. L2 regularization\n",
        "  $$\n",
        "  \\begin{split}\n",
        "    Regularized Loss &= Original loss + regularization\\\\\n",
        "    &=\\dfrac{1}{m} ∑(y_i - θ^T x_i)^2 + γ ∑ |θ_j|^2\n",
        "  \\end{split}\n",
        "  $$\n",
        "  L2 spreads the numbers \n",
        "  \n",
        "  $γ$ is regularization weigth\n",
        "\n",
        "  ## Regression variants\n",
        "\n",
        "  1. **Ridge regression** : regression with L2 regularization\n",
        "  1. **Lasso regression** : regression with L1 regularization\n",
        "  1. **ElasticNet regression** : regression with L1 and L2 regularization"
      ],
      "metadata": {
        "id": "77id74HQZfjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic function"
      ],
      "metadata": {
        "id": "e9oLR6sXdwP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- let's force h to be between 0 and 1 somehow\n",
        "- Introducing the logistic function (sigmoid function)\n",
        "![sigmiod](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png)\n",
        "\n",
        "pass model linear regression ( $θ^Tx$ ) trough the logistic function\n",
        "\n",
        "$$\n",
        "  h_θ(x) = \\dfrac{1}{1+e^{-θ^Tx}}\n",
        "$$\n",
        "\n",
        "used in **binary classification**"
      ],
      "metadata": {
        "id": "bhdfI67FdyOw"
      }
    }
  ]
}