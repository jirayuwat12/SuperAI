{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzHX0pZVTKabZl3nMgEmTs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jirayuwat12/SuperAI/blob/master/Introduction_to_Classical_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## types of machine learning \n",
        "\n",
        "1. supervised\n",
        "1. unsupervised\n",
        "1. reinforcement"
      ],
      "metadata": {
        "id": "zAasZDWtJazU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "lyfgTkcGYTfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Metrics\n",
        "- compare the output of the models\n",
        "  - errors/failures, accuracy/success\n",
        "- we want to quantify the error/accuracy of the models\n",
        "- how would you measure the error/accuracy of the following\n",
        "\n",
        "ex. AUC, Confusion, FA FR, Error, Door open correctly\n",
        "\n",
        "1. Error rate\n",
        "1. Accuracy rate\n",
        "1. Precision\n",
        "1. True positive\n",
        "1. Recall\n",
        "1. False alarm\n",
        "1. F score"
      ],
      "metadata": {
        "id": "8B4hJNExNhcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Supervised learning"
      ],
      "metadata": {
        "id": "QeHEj6Z0W0Ny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear regression"
      ],
      "metadata": {
        "id": "_7vGaKsSYP-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "eg. predicting housing price\n",
        "\n",
        "$h_θ (x_1) = θ_0 + θ_1x_{1,1}+ θ_2x_{1,2}+...  $\n",
        "\n",
        "$h_θ (x_1) = ∑ θ^Tx_i$\n",
        "\n",
        "θs are the parameter (or **weigths**)\n",
        "\n",
        "- cost/loss function\n",
        "\n",
        "  1. MSE ( mean square error ) or L2 loss\n",
        "\n",
        "  $ J(θ) = \\dfrac{1}{m} ∑ (y_i - θ^T x_i)^2 $\n",
        "\n",
        "  2. L1 loss\n",
        "\n",
        "  $ J(θ) = \\dfrac{1}{m} ∑ |y_i - θ^T x_i| $\n",
        "\n",
        "- gradient descent \n",
        "  \n",
        "  1. y = f(x)\n",
        "  1. pick start point $x_0$\n",
        "  1. move along $-\\dfrac{dy}{dx}$\n",
        "  1. $x_{n+1} = x_n - r \\times \\dfrac{dy}{dx}$\n",
        "  1. repeat till convergence\n",
        "  \n",
        "  <u>note</u> r = learning rate\n",
        "\n",
        "  - gradient of loss function\n",
        "$\n",
        "    \\dfrac{∂J}{∂θ_j} = - ∑(y_i - θ^T x_i)x_i^{(j)}\n",
        "$\n",
        "\n",
        "$\n",
        "  hence, θ_{j+1} \\impliedby θ_j + r ∑ (y_i - θ^T x_i)x_i^{(j)}\n",
        "$"
      ],
      "metadata": {
        "id": "CTcaIkHKQzaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression with non-linear features"
      ],
      "metadata": {
        "id": "W4SGnvVxZaSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "eg. predicting housing price\n",
        "\n",
        "$h_θ (x_1) = θ_0 + θ_1x_{1,1}+ θ_2x_{1,2}+... +θ_nx_{1,1}^2+θ_{n+1}x_{1,2}^2 + ... $\n",
        "\n",
        "θs are the parameter (or **weigths**)\n",
        "\n",
        "can cause **Overfitting and Underfitting**\n",
        "\n",
        "- reducing overfiting with regularization\n",
        "\n",
        "  1. L1 regularization\n",
        "  $$\n",
        "  \\begin{split}\n",
        "    Regularized Loss &= Original loss + regularization\\\\\n",
        "    &=\\dfrac{1}{m} ∑(y_i - θ^T x_i)^2 + γ ∑ |θ_j|\n",
        "  \\end{split}\n",
        "  $$\n",
        "  L1 does feature selection ( most number be 0 )\n",
        "  2. L2 regularization\n",
        "  $$\n",
        "  \\begin{split}\n",
        "    Regularized Loss &= Original loss + regularization\\\\\n",
        "    &=\\dfrac{1}{m} ∑(y_i - θ^T x_i)^2 + γ ∑ |θ_j|^2\n",
        "  \\end{split}\n",
        "  $$\n",
        "  L2 spreads the numbers \n",
        "  \n",
        "  $γ$ is regularization weigth\n",
        "\n",
        "  ## Regression variants\n",
        "\n",
        "  1. **Ridge regression** : regression with L2 regularization\n",
        "  1. **Lasso regression** : regression with L1 regularization\n",
        "  1. **ElasticNet regression** : regression with L1 and L2 regularization"
      ],
      "metadata": {
        "id": "77id74HQZfjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic function"
      ],
      "metadata": {
        "id": "e9oLR6sXdwP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- let's force h to be between 0 and 1 somehow\n",
        "- Introducing the logistic function (sigmoid function)\n",
        "![sigmiod](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png)\n",
        "\n",
        "pass model linear regression ( $θ^Tx$ ) trough the logistic function\n",
        "\n",
        "$$\n",
        "  h_θ(x) = \\dfrac{1}{1+e^{-θ^Tx}}\n",
        "$$\n",
        "\n",
        "used in **binary classification**"
      ],
      "metadata": {
        "id": "bhdfI67FdyOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unsepervised learning\n",
        "\n",
        "try to find insign of data"
      ],
      "metadata": {
        "id": "fY9qJBEcW5hB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-mean clustering\n",
        "\n",
        "clustering - task tries to automatically discover groups within the data\n",
        "\n",
        "- nearest neighbour classification\n",
        "  \n",
        "  finding the closest training data, assign the same label as the training data **but very sensitive to outlier**\n",
        "\n",
        "- K-nearest neighbour classification\n",
        "\n",
        "  use a voting scheme instead\n",
        "\n",
        "cons\n",
        "- cant use with high dimention\n",
        "- slow if many data **fix with centroid**\n",
        "- need to choose **K** first\n",
        "\n",
        "\n",
        "selecting-K\n",
        "1. uing Elbow method\n",
        "$$\n",
        "  fraction\\ of\\ explained\\ variance = \\dfrac{between\\ cluster\\ variance}{all\\ data\\ variance}\n",
        "$$\n",
        "$$\n",
        "  between\\ cluster\\ variance = \\sum\\dfrac{n_i(M_i-M)^2}{N-1}\n",
        "$$\n",
        "$$\n",
        "  all\\ data\\ variance = \\sum\\dfrac{(x_i-M)^2}{N-1}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$n_i$ is size of $i^{th}$ cluster,\n",
        "\n",
        "$M_i$ is centroid of $i^{th}$ cluster\n",
        "\n",
        "$M$ is all data centroid\n",
        "\n",
        "then choose the suitable K, from graph of K and fraction of expained variance"
      ],
      "metadata": {
        "id": "xf47txmbW_LZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GUVilDAE06VI"
      }
    }
  ]
}